{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "import requests\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from listings_consumer import get_shard_iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def c(x): \n",
    "    if x:\n",
    "        return x.text.replace('\\r', '').replace('\\t', '').replace('\\n', ' ').strip()\n",
    "    return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_pages(pages):\n",
    "    \"Return parsed pages\"\n",
    "    results = []\n",
    "\n",
    "    for page in range(pages):\n",
    "        print(f'Getting page {page+1} from looperghana')\n",
    "        results.append(requests.get(f\"https://listings.loopghana.com/pageNumber_{page}\").text)\n",
    "\n",
    "    soups = []\n",
    "    for r in results:\n",
    "        soup = BeautifulSoup(r, 'lxml')\n",
    "        soups.append(soup)\n",
    "    \n",
    "    return soups    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_listings(soups):\n",
    "    \"Extract listings from list of beautiful soup objects\"\n",
    "    \n",
    "    data = []\n",
    "\n",
    "    for soup in soups:\n",
    "        listings = soup.find_all('ul', {'class': 'listings-list'})[1].find_all('li')\n",
    "\n",
    "        for prop in listings:\n",
    "            d = {\n",
    "                'broker': c(prop.find('span', {'class': 'agt'})),\n",
    "                'category': prop.find('a').attrs['data-id'],\n",
    "                'price': c(prop.find('span', {'class': 'price'})),\n",
    "                'area': c(prop.find('span', {'class': 'size'})), \n",
    "                'beds': c(prop.find('span', {'class': 'bedrooms'})),\n",
    "                'bath': c(prop.find('span', {'class': 'bathrooms'})),\n",
    "                'url': prop.find('a').attrs['href'],\n",
    "\n",
    "            }\n",
    "            data.append(d)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(df):\n",
    "    df['currency'] = df.price.apply(lambda x: re.match(r'[A-Z\\$]+', x.replace(',', '')).group())\n",
    "    df['price'] = df.price.apply(lambda x: re.findall(r'[0-9]+', x.replace(',', ''))[0])\n",
    "    df['area'] = df['area'].str.replace('m2', '')\n",
    "    df['beds'] = df['beds'].str.replace('Bed', '')\n",
    "    df['bath'] = df['bath'].str.replace('Bath', '')\n",
    "    df['source'] = 'loopghana'\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coords(url):\n",
    "    \n",
    "    try:\n",
    "        results = requests.get(url)\n",
    "        soup = BeautifulSoup(results.text, 'lxml')\n",
    "        js = soup.find_all('script', {'type': \"text/javascript\"})[0]\n",
    "\n",
    "        coords = re.findall(r\"(ws_l[a-z]+ = '-?[0-9].[0-9]+')\", js.text)\n",
    "\n",
    "        if coords:\n",
    "            coords = dict(map(lambda x: x.split(' = '), coords))\n",
    "            lat = coords.get('ws_lat', None)\n",
    "            lon = coords.get('ws_lon', None)\n",
    "\n",
    "        return lat, lon\n",
    "    except:\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enrich(df):\n",
    "    df['lat'] = None\n",
    "    df['lon'] = None\n",
    "    df[['lat', 'lon']] = df.url.apply(lambda x: get_coords(x)).apply(pd.Series)\n",
    "\n",
    "    df['lat'] = df['lat'].apply(lambda x: x[1:-1] if x else None).astype(float)\n",
    "    df['lon'] = df['lon'].apply(lambda x: x[1:-1] if x else None).astype(float)\n",
    "    \n",
    "#     df = df[['id', 'location', 'currency', 'price', 'area', 'bedrooms', 'bathrooms', 'url', 'lat', 'lon']]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_looper(pages=4, add_gps=False):\n",
    "    soups = parse_pages(pages)\n",
    "    data = extract_listings(soups)\n",
    "    df = pd.DataFrame(data)\n",
    "    df = clean(df)\n",
    "    \n",
    "    if enrich:\n",
    "        df = enrich(df)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting page 1 from looperghana\n",
      "Getting page 2 from looperghana\n",
      "Getting page 3 from looperghana\n",
      "Getting page 4 from looperghana\n"
     ]
    }
   ],
   "source": [
    "df = scrape_looper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
